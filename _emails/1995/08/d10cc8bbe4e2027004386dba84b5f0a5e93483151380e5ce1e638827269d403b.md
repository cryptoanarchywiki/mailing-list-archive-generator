---
layout: default
---

# 1995-08-27 - Re: SSL trouble

## Header Data

From: Piete Brooks \<Piete.Brooks<span>@</span>cl.cam.ac.uk\><br>
To: Christian Wettergren \<cwe@Csli.Stanford.EDU\><br>
Message Hash: d10cc8bbe4e2027004386dba84b5f0a5e93483151380e5ce1e638827269d403b<br>
Message ID: \<"swan.cl.cam.:079000:950827192056"@cl.cam.ac.uk\><br>
Reply To: \<199508271848.LAA18104@Csli.Stanford.EDU\><br>
UTC Datetime: 1995-08-27 19:21:34 UTC<br>
Raw Date: Sun, 27 Aug 95 12:21:34 PDT<br>

## Raw message

```
{% raw  %}From: Piete Brooks <Piete.Brooks@cl.cam.ac.uk>
Date: Sun, 27 Aug 95 12:21:34 PDT
To: Christian Wettergren <cwe@Csli.Stanford.EDU>
Subject: Re: SSL trouble
In-Reply-To: <199508271848.LAA18104@Csli.Stanford.EDU>
Message-ID: <"swan.cl.cam.:079000:950827192056"@cl.cam.ac.uk>
MIME-Version: 1.0
Content-Type: text/plain


> And there is the third alternative, hierarchical search, which
> distributes the task of giving out keys. This is admittedly a
> little bit more involved, of course. The SKSP had provisions for
> doing it hierarchically, as far as I understood it, although
> I might be wrong.

Indeed, it does, and we plan to provide a "local CPU farm" server
which can be used when a number of machine are sharing the same ID.

> What I wonder is wheter the server congestion really showed that
> the protocol is flawed.

No -- but that the early version of the code were buggy.

As it is, 6 clients which are still running are managing to keep the
server permanently busy.

I think the protocol itself is OKish ..

> Handing out bigger blocks relieved the  situation.

Not really.

It did however mean that when a chunk was allocated, three times as
much work was done !


> 1. The server knows approximately how many requests per second it 
> can take, and tells the clients this information.

Hmm -- hard to tell -- the *server* can take lots, but if the
*clients* have problems, things go wrong.

A select/poll server is not going to be tried on the next one -- that'll only
be used if that goes slow as well ...

> 2. The client initially does a testrun, and determines how fast it 
> runs.

The latest version of brloop starts with a call of "brutessl -q -t 1"
to decide how big the chunks should be ...

> 3. Each client is handed a block that, given the approximate number
> of currently pending and active blocks out there, together with the
> calculation time of the client, will give an acceptable number of 
> requests/time unit to the server.

I suspect that figures would be too crude ...
The server would have to keep track of clients and how long their
sessions take ....
Should a client which takes 20s for a session be given blocks that take
20 times longer to process than one which manages it in 1s ?

> 4. The server acks (S-ACK) the block-ack to the client.

Sorry -- what does that mean ?

> If the client doesn't get an ack (S-ACK) from the server for its ack (B-ACK),
> it keeps the ack around til the next block is calculated, and sends this 
> ack together with the new acks.

Sorry -- I'm lost ...

> 5. The server can hand out allocated blocks to others, for those
> blocks that has not been acked in three times the estimated
> calculation time. 

I've split allocation from ACKs.
One server just doles out keys, the other just collects the ACKs.
I don't want to add that sort of realtime feedback.

What do you do about WWW clients ?

What if someone grabe a big chunk, farms it out to several machines,
and they ACK bits back ... ?


> 6. If a client is unable to get a key allocation after a number of 
> tries, it can chose a random block and search that. It can then be
> acked to the server. This may result in overlapping blocks, but this
> should not pose such a big problem, since most of the key space is
> searched in an orderly manner anyway.

Again no realtime fedback from ACKs :-(

> It would be very interesting if detailed statistics or the logfile
> of the server could be published somewhere. How many machines were
> involved? etc...

That'll come -- as the WWW pags says. pelase let me know what stats you'd like.




{% endraw %}
```

## Thread

+ Return to [August 1995](/archive/1995/08)

+ Return to "[Christian Wettergren <cwe<span>@</span>Csli.Stanford.EDU>](/author/christian_wettergren_cwe_at_csli_stanford_edu_)"
+ Return to "[Damien.Doligez<span>@</span>inria.fr (Damien Doligez)](/author/damien_doligez_at_inria_fr_damien_doligez_)"
+ Return to "[hallam<span>@</span>w3.org](/author/hallam_at_w3_org)"
+ Return to "[Nathan Loofbourrow <loofbour<span>@</span>cis.ohio-state.edu>](/author/nathan_loofbourrow_loofbour_at_cis_ohiostate_edu_)"
+ Return to "[Piete Brooks <Piete.Brooks<span>@</span>cl.cam.ac.uk>](/author/piete_brooks_piete_brooks_at_cl_cam_ac_uk_)"

+ 1995-08-26 (Sat, 26 Aug 95 03:34:18 PDT) - [SSL trouble](/archive/1995/08/0fcc6aa93df5545d5cc9ffd3661b3ab4d8bded88ec1d14b81f68f85c9eb23f26) - _Damien.Doligez@inria.fr (Damien Doligez)_
  + 1995-08-26 (Sat, 26 Aug 95 09:40:40 PDT) - [Re: SSL trouble](/archive/1995/08/ea2c60f7a9d5d77ad00b3479ffe2c760584d78c656aafc45e1c574d3928c95d6) - _Piete Brooks \<Piete.Brooks@cl.cam.ac.uk\>_
  + 1995-08-26 (Sat, 26 Aug 95 10:59:28 PDT) - [Re: SSL trouble](/archive/1995/08/46ad7e461c8c368595d56db4868e827bcfb0a1b0a9bf9ecd063fc3aa28ff7346) - _hallam@w3.org_
  + 1995-08-27 (Sun, 27 Aug 95 11:48:30 PDT) - [Re: SSL trouble](/archive/1995/08/70103cec27835829a44cd918ec700d7e3a64c025522c0bb8f1d854ea515f9fb8) - _Christian Wettergren \<cwe@Csli.Stanford.EDU\>_
    + 1995-08-27 (Sun, 27 Aug 95 12:21:34 PDT) - Re: SSL trouble - _Piete Brooks \<Piete.Brooks@cl.cam.ac.uk\>_
    + 1995-08-28 (Sun, 27 Aug 95 18:52:16 PDT) - [SSL ACKs vs. Anonymity (was Re: SSL trouble)](/archive/1995/08/a1c752b8d218444116316c2b98f378c2d8ddf80cdb2af40fc0f5511e93fe85a9) - _Nathan Loofbourrow \<loofbour@cis.ohio-state.edu\>_

